# ğŸ“¢ AI Chat with Ollama & DeepSeek-R1:7B

A simple Ollama-based `Gemma3` chatbot using **Node.js** and **Express**

![Chat UI](preview.png)
*Example chat UI*

## âœ¨ Features
âœ… **Communication with a local LLM through a Web-UI**
âœ… **Modern Minimal Dark-mode Design**
âœ… **Node.js Backend** with Express.js

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ **Install Ollama & Pull the Model**
Ensure you have [Ollama](https://ollama.com/) installed and run the following command to download the `gemma3:4b` (or any other) model:
```sh
ollama pull gemma3:4b
```

### 2ï¸âƒ£ **Clone the Repository**
```sh
git clone https://github.com/aquastones/ollama-webui.git
cd ollama-webui
```

### 3ï¸âƒ£ **Install Dependencies**
```sh
npm install
```

### 4ï¸âƒ£ **Run the Ollama Model**
```sh
ollama run gemma3:4b
```

### 5ï¸âƒ£ **Start the Server**
```sh
node server.js
```

### 6ï¸âƒ£ **Open the Chat in Your Browser**
Go to:
```
http://localhost:3000
```

---

## ğŸ“œ How It Works
1ï¸âƒ£ **Frontend (HTML/CSS/JS)**
- Minimal UI with a **chatbox** and input field.
- Sends user messages to the backend.

2ï¸âƒ£ **Backend (Node.js + Express)**
- Handles chat requests from the frontend.
- Calls Ollama's API to generate responses.

## ğŸ”§ Frameworks Used
- **Node.js** (Backend)
- **Express.js** (Server)
- **Ollama** (AI Model API)
- **JavaScript** (Frontend)
- **HTML + CSS** (UI Design)

---

## ğŸ› ï¸ Customization
Want to use a different model? Change the `model` name in `server.js`:
```js
const response = await axios.post("http://localhost:11434/api/generate", {
    model: "your-model-name",  // Change model here
    prompt: message,
    stream: false
});
```

---

## ğŸ“œ License
This project is open-source under the **MIT License**.

---

## ğŸŒŸ Contributing
Got an idea? Found a bug? Feel free to **fork the repo** and submit a **pull request**!

---

## ğŸ“¬ Contact
- **GitHub**: [aquastones](https://github.com/aquastones)
- **Email**: aquastones@icloud.com